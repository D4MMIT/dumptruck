{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "    \n",
    "    #ax2.xaxis.tick_top()\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    #ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(SkipEnv, self).__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, t_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer = []\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "class PreProcessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(PreProcessFrame, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "                                                shape=(80,80,1), dtype=np.uint8)\n",
    "    def observation(self, obs):\n",
    "        return PreProcessFrame.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "\n",
    "        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n",
    "\n",
    "        new_frame = 0.299*new_frame[:,:,0] + 0.587*new_frame[:,:,1] + \\\n",
    "                    0.114*new_frame[:,:,2]\n",
    "\n",
    "        new_frame = new_frame[35:195:2, ::2].reshape(80,80,1)\n",
    "\n",
    "        return new_frame.astype(np.uint8)\n",
    "\n",
    "class MoveImgChannel(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MoveImgChannel, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                            shape=(self.observation_space.shape[-1],\n",
    "                                   self.observation_space.shape[0],\n",
    "                                   self.observation_space.shape[1]),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaleFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
    "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
    "                             dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = SkipEnv(env)\n",
    "    env = PreProcessFrame(env)\n",
    "    env = MoveImgChannel(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaleFrame(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2781</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.009623</td>\n",
       "      <td>961067.10</td>\n",
       "      <td>13407557.82</td>\n",
       "      <td>2024-03-23T00:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2781</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>974010.65</td>\n",
       "      <td>13717864.69</td>\n",
       "      <td>2024-03-23T03:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2781</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>1030381.52</td>\n",
       "      <td>13987644.47</td>\n",
       "      <td>2024-03-23T06:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2781</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>838887.07</td>\n",
       "      <td>13875271.43</td>\n",
       "      <td>2024-03-23T09:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2781</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.009369</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>646186.26</td>\n",
       "      <td>13078427.75</td>\n",
       "      <td>2024-03-23T12:00:00.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name      open      high       low     close      volume    marketCap  \\\n",
       "0  2781  0.009883  0.009953  0.009156  0.009623   961067.10  13407557.82   \n",
       "1  2781  0.009621  0.010043  0.009453  0.009845   974010.65  13717864.69   \n",
       "2  2781  0.009842  0.010040  0.009706  0.010039  1030381.52  13987644.47   \n",
       "3  2781  0.010002  0.010065  0.009477  0.009968   838887.07  13875271.43   \n",
       "4  2781  0.009958  0.009991  0.009369  0.009386   646186.26  13078427.75   \n",
       "\n",
       "                  timestamp  \n",
       "0  2024-03-23T00:00:00.000Z  \n",
       "1  2024-03-23T03:00:00.000Z  \n",
       "2  2024-03-23T06:00:00.000Z  \n",
       "3  2024-03-23T09:00:00.000Z  \n",
       "4  2024-03-23T12:00:00.000Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/TAMA_7D_graph_coinmarketcap.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DQN, self).__init__() #constructor for base class\n",
    "        self.input_dims = input_dims\n",
    "        self.lr = lr\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims) #* unpacks list, input observation vectors\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss=nn.MSELoss() #Q learning is like linear regression\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu') \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, max_mem_size=100000, eps_end = 0.01, eps_dec=5e-4):\n",
    "        # gamma = awaiting future rewards, epsilon: how often to explore or take action\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon #porp of time taking random or greedy option policy\n",
    "        self.lr = lr\n",
    "        self.input_dims= input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.mem_size = max_mem_size\n",
    "        self.action_space = [ i for i in range(n_actions)]\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.Q_eval = DQN(self.lr, n_actions=self.n_actions, input_dims=self.input_dims, \n",
    "                          fc1_dims=256, fc2_dims =256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        #deep q doesn't work for continuous action spaces\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool) #mask for setting\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index]  = reward\n",
    "        self.action_memory[index] = action #one hot encoding for actions? action to int one hot back to int one hot = [0,1,0,0]\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr+=1\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([obs]).to(self.Q_eval.device) #bracket around obs because setup\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        #dilemma: mem filled with zero how to deal with that cant learn from zero\n",
    "        # 1. play games rand until fill memory then learn? 2. start learning as soon as filled batch size\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return # dont bother learning if not big enough\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad() #zero gradient, \n",
    "        #select up to max filled memory\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem,self.batch_size, replace=False ) #not problem if a huge batch size/memory storage\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32) #need this?\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)#subset of agent mem \n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch= T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        action_batch = self.action_memory[batch]\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch] #values for action we actually took for each set\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        #target network?\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch+self.gamma*T.max(q_next, dim=1)[0] # discount factor gamma, max value of next state, greedy action\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimzer.step()\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o-o\\AppData\\Roaming\\Python\\Python312\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m obs_ , reward, done, info, _m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     16\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 17\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[0;32m     19\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs_\n",
      "Cell \u001b[1;32mIn[12], line 53\u001b[0m, in \u001b[0;36mAgent.store_transition\u001b[1;34m(self, state, action, reward, state_, done)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_transition\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, state_, done):\n\u001b[0;32m     52\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_cntr \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_size\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_state_memory[index] \u001b[38;5;241m=\u001b[39m state_\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_memory[index]  \u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size=64, n_actions=4, \n",
    "              eps_end=0.01, input_dims=[8], lr=0.03)\n",
    "scores, eps_history = [], []\n",
    "n_games=500\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        obs_ , reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(obs, action, reward, obs_, done)\n",
    "        agent.learn()\n",
    "        obs = obs_\n",
    "\n",
    "    \n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    avg_score = np.mean(score[-100:]) #avg of previous 100 games\n",
    "\n",
    "    print('episode', i, 'score %.2f' % score,\n",
    "          'average score %.2f' % avg_score,\n",
    "          'epsilon %.2f' % agent.epsilon)\n",
    "    \n",
    "x = [i+1 for i in range(n_games)]\n",
    "fn = 'lunar_lander.png'\n",
    "plotLearning(x, scores, eps_history, fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
